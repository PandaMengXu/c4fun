#include <time.h>
#include <stdio.h>
#include <stdint.h>
#include <stdlib.h>
#include <unistd.h>
#include <numa.h>
#include <assert.h>
#include <sys/time.h>
#include <errno.h>
#include <err.h>
#include <sys/mman.h>

// For thread affinity
#include <sched.h>

#include <sys/ioctl.h>

#include "pebs_bench.h"
#include "pebs_bench_ui.h"

#define CPU 8
#define NUMA_NODE 0
#define NUMA_ALLOC 1 // Set to one to use numa_alloc

#define ELEM_TYPE uint64_t


static long perf_event_open(struct perf_event_attr *hw_event, pid_t pid, int cpu, int group_fd, unsigned long flags) {
    int ret = syscall(__NR_perf_event_open, hw_event, pid, cpu,
                   group_fd, flags);
    return ret;
}

/**
 * Fills a memory region of the given size with values indicating the
 * adress of the next element in the memory region.
 */
void fill_memory_sequential(size_t size, uint64_t* memory, size_t nb_elems) {

  int i;
  for(i = 0; i < nb_elems - 1; i++) {
    memory[i] = (uint64_t)&memory[i + 1];
  }
  memory[i] = (uint64_t)&memory[0];
}


/**
 * Structure used along with the following compar function to shuffle
 * an array of N elements.
 */
struct rand_struct {
  int index;
  int rand;
};

static int compar(const void* a1, const void* a2) {
  struct rand_struct *a = (struct rand_struct*) a1;
  struct rand_struct *b = (struct rand_struct*) a2;
  return a->rand - b->rand;
}

/**
 * Fills a memory region of the given size with values indicating the
 * adress an other random element in the memory region. All the
 * elements adresses are present in the filled region.
 */
void fill_memory_rand(size_t size, ELEM_TYPE *memory, size_t nb_elems) {

  /**
   * Create another memory region and shuffle it.
   */
  size_t rand_memory_size = nb_elems * sizeof(struct rand_struct);
  struct rand_struct *rand_memory = malloc(rand_memory_size);
  memset(rand_memory, 0, rand_memory_size);
  assert(rand_memory);
  unsigned int seed = 1;
  for (int i = 0; i < nb_elems; i++) {
    rand_memory[i].index = i;
    rand_memory[i].rand = rand_r(&seed);
  }
  qsort(&rand_memory[1], nb_elems - 1, sizeof(*rand_memory), compar);

  /**
   * Fills the returned memory region with pointers to a next random
   * element using the shuffled memory region.
   */
  int i;
  /* int nb_cache_line_changes = 0; */
  for(i = 0; i < nb_elems - 1; i++) {
    memory[i] = (uint64_t)&memory[rand_memory[i + 1].index];
    /* if (i > 0 && memory[i] - memory[i - 1] > 64) { */
    /*   nb_cache_line_changes++; */
    /* } */
  }
  /* printf("nb_cache_line_changes = %d\n", nb_cache_line_changes); */
  memory[i] = (uint64_t)&memory[0];
  free(rand_memory);
}

#define ONE addr = (uint64_t *)*addr;
#define FOUR ONE ONE ONE ONE
#define SIXTEEN FOUR FOUR FOUR FOUR
#define THIRTY_TWO SIXTEEN SIXTEEN
#define SIXTY_FOUR THIRTY_TWO THIRTY_TWO

/**
 * Read the given memory region. Several accesses are done in each
 * loop iteration to limit the number of accesses caused by the loop
 * test.
 */
void read_memory(uint64_t *memory, size_t size) {

  // register allocation to avoid having memory loads generated by
  // stack accesses. If addr is in the stack, the line addr =
  // (uint64_t *)*addr is compiled (in pseudo anssembler) to r1 =
  // mem[addr]; r2 = mem[r1]; mem[addr] = r2; When doing the register
  // allocation, the code is ocmpiled to: r2 = mem[r1]; r1 = r2; and
  // we see that in that case we only have one memory load.
  register uint64_t *addr = memory;

  // Also register allocation for the same reason
  register int64_t nb_elem_remaining = size / sizeof(ELEM_TYPE);

  while (nb_elem_remaining > 0) {
    THIRTY_TWO
    nb_elem_remaining -= 32;
  }
}

enum access_mode_t {
  access_seq,
  access_random
};


int run_benchs(size_t size_in_bytes, enum access_mode_t access_mode, uint64_t period) {

  int nb_elems = size_in_bytes / sizeof(ELEM_TYPE);
  printf("Accessing %lu mega bytes in %d %ld bytes accesses\n", (size_in_bytes / 1000000), nb_elems, sizeof(ELEM_TYPE));

  /**
   * Allocates and fills memory. Because the memory is filled, all its
   * pages are touched and as a consequence, no page faults will occur
   * during the measurement.
   */
  numa_available();
  uint64_t *memory;
  if (numa_available() != -1 && NUMA_ALLOC) {
    memory = numa_alloc_onnode(size_in_bytes, NUMA_NODE);
  } else {
    memory = malloc(size_in_bytes);
  }
  assert(memory);
  printf("Memory is betweent %p and %p\n", memory, memory + nb_elems);
  memset(memory, -1, size_in_bytes);
  if (access_mode == access_seq) {
    fill_memory_sequential(size_in_bytes, memory, nb_elems);
  } else {
    fill_memory_rand(size_in_bytes, memory, nb_elems);
  }
  mlockall(MCL_CURRENT | MCL_FUTURE); // Ensure pages are not swapped

  /**
   * Profile memory and other things
   */
  struct perf_event_attr pe_attr_page_faults;
  memset(&pe_attr_page_faults, 0, sizeof(pe_attr_page_faults));
  pe_attr_page_faults.size = sizeof(pe_attr_page_faults);
  pe_attr_page_faults.type =   PERF_TYPE_SOFTWARE;
  pe_attr_page_faults.config = PERF_COUNT_SW_PAGE_FAULTS;
  pe_attr_page_faults.disabled = 1;
  pe_attr_page_faults.exclude_kernel = 1;
  int page_faults_fd = perf_event_open(&pe_attr_page_faults, 0, CPU, -1, 0);
  if (page_faults_fd == -1) {
    printf("perf_event_open failed for page faults: %s\n", strerror(errno));
    return -1;
  }

  // Manualy set and open 64 bytes cache line reads from memory counting event
  struct perf_event_attr pe_attr_unc_memory;
  memset(&pe_attr_unc_memory, 0, sizeof(pe_attr_unc_memory));
  pe_attr_unc_memory.size = sizeof(pe_attr_unc_memory);
  pe_attr_unc_memory.type = 6; // /sys/bus/event_source/devices/uncore/type
  pe_attr_unc_memory.config = 0x072c; // QMC_NORMAL_READS.ANY
  pe_attr_unc_memory.disabled = 1;
  int memory_reads_fd = perf_event_open(&pe_attr_unc_memory, -1, NUMA_NODE, -1, 0);
  if (memory_reads_fd == -1) {
    printf("perf_event_open failed for uncore memory: %s\n", strerror(errno));
    return -1;
  }

  // Manualy set and open load retired counting event
  struct perf_event_attr pe_attr_loads;
  memset(&pe_attr_loads, 0, sizeof(pe_attr_loads));
  pe_attr_loads.size = sizeof(pe_attr_loads);
  pe_attr_loads.type = PERF_TYPE_RAW;
  pe_attr_loads.config = 0x010b; // MEM_INST_RETIRED.LOADS
  pe_attr_loads.disabled = 1;
  pe_attr_loads.exclude_kernel = 1;
  pe_attr_loads.exclude_hv = 1;
  int loads_fd = perf_event_open(&pe_attr_loads, 0, CPU, -1, 0);
  if (loads_fd == -1) {
    printf("perf_event_open failed for core loads: %s\n", strerror(errno));
    return -1;
  }

  // Manualy set and open instructions retired counting event
  struct perf_event_attr pe_attr_inst;
  memset(&pe_attr_inst, 0, sizeof(pe_attr_inst));
  pe_attr_inst.size = sizeof(pe_attr_inst);
  pe_attr_inst.type = PERF_TYPE_RAW;
  pe_attr_inst.config = 0x00c0; // INST_RETIRED.ANY
  pe_attr_inst.disabled = 1;
  pe_attr_inst.exclude_kernel = 1;
  pe_attr_inst.exclude_hv = 1;
  int inst_fd = perf_event_open(&pe_attr_inst, 0, CPU, -1, 0);
  if (inst_fd == -1) {
    printf("perf_event_open failed for instructions: %s\n", strerror(errno));
    return -1;
  }

  // Manualy set and open remote cache counting event
  struct perf_event_attr pe_attr_remote_cache;
  memset(&pe_attr_remote_cache, 0, sizeof(pe_attr_remote_cache));
  pe_attr_remote_cache.size = sizeof(pe_attr_remote_cache);
  pe_attr_remote_cache.type = PERF_TYPE_RAW;
  pe_attr_remote_cache.config = 0x5301b7; // OFF_CORE_RESPONSE_0
  pe_attr_remote_cache.config1 = 0x1033; // REMOTE_CACHE_ FWD
  pe_attr_remote_cache.disabled = 1;
  pe_attr_remote_cache.exclude_kernel = 1;
  pe_attr_remote_cache.exclude_hv = 1;
  int remote_cache_fd = perf_event_open(&pe_attr_remote_cache, 0, CPU, -1, 0);
  if (remote_cache_fd == -1) {
    printf("perf_event_open failed for remote cache: %s\n", strerror(errno));
    return -1;
  }

  // Manualy set and open memory sampling event
  struct perf_event_attr pe_attr_sampling;
  memset(&pe_attr_sampling, 0, sizeof(pe_attr_sampling));
  pe_attr_sampling.size = sizeof(pe_attr_sampling);
  pe_attr_sampling.type = PERF_TYPE_RAW;
  pe_attr_sampling.config = 0x100b; // MEM_INST_RETIRED.LATENCY_ABOVE_THRESHOLD
  pe_attr_sampling.disabled = 1;
  pe_attr_sampling.config1 = 3; // latency threshold
  pe_attr_sampling.sample_period = period;
  pe_attr_sampling.precise_ip = 2;
  pe_attr_sampling.sample_type = PERF_SAMPLE_IP | PERF_SAMPLE_ADDR | PERF_SAMPLE_WEIGHT | PERF_SAMPLE_DATA_SRC;
  pe_attr_sampling.exclude_kernel = 1;
  int memory_sampling_fd = perf_event_open(&pe_attr_sampling, 0, CPU, -1, 0);
  if (memory_sampling_fd == -1) {
    printf("perf_event_open failed for sampling: %s\n", strerror(errno));
    return -1;
  }
  long page_size = sysconf(_SC_PAGESIZE);
  size_t mmap_len = page_size + page_size * 1024;
  struct perf_event_mmap_page *metadata_page = mmap(NULL, mmap_len, PROT_WRITE, MAP_SHARED, memory_sampling_fd, 0);
  if (metadata_page == MAP_FAILED) {
    fprintf (stderr, "Couldn't mmap file descriptor: %s - errno = %d\n",
  	     strerror (errno), errno);
    return -1;
  }

  // Starts measuring
  struct timeval t1, t2;
  double elapsedTime;
  gettimeofday(&t1, NULL);
  ioctl(memory_reads_fd, PERF_EVENT_IOC_RESET, 0);
  ioctl(memory_reads_fd, PERF_EVENT_IOC_ENABLE, 0);
  ioctl(loads_fd, PERF_EVENT_IOC_RESET, 0);
  ioctl(loads_fd, PERF_EVENT_IOC_ENABLE, 0);
  ioctl(inst_fd, PERF_EVENT_IOC_RESET, 0);
  ioctl(inst_fd, PERF_EVENT_IOC_ENABLE, 0);
  ioctl(page_faults_fd, PERF_EVENT_IOC_RESET, 0);
  ioctl(page_faults_fd, PERF_EVENT_IOC_ENABLE, 0);
  ioctl(memory_sampling_fd, PERF_EVENT_IOC_RESET, 0);
  ioctl(memory_sampling_fd, PERF_EVENT_IOC_ENABLE, 0);
  ioctl(remote_cache_fd, PERF_EVENT_IOC_RESET, 0);
  ioctl(remote_cache_fd, PERF_EVENT_IOC_ENABLE, 0);


  // Access memory
  read_memory(memory, size_in_bytes);

  // Stop measuring
  ioctl(remote_cache_fd, PERF_EVENT_IOC_DISABLE, 0);
  ioctl(memory_sampling_fd, PERF_EVENT_IOC_DISABLE, 0);
  ioctl(page_faults_fd, PERF_EVENT_IOC_DISABLE, 0);
  ioctl(inst_fd, PERF_EVENT_IOC_DISABLE, 0);
  ioctl(loads_fd, PERF_EVENT_IOC_DISABLE, 0);
  ioctl(memory_reads_fd, PERF_EVENT_IOC_DISABLE, 0);
  gettimeofday(&t2, NULL);
  elapsedTime = (t2.tv_sec - t1.tv_sec) * 1000.0;
  elapsedTime += (t2.tv_usec - t1.tv_usec) / 1000.0;

  // Print results
  uint64_t remote_cache_count;
  uint64_t memory_reads_count;
  uint64_t loads_count;
  uint64_t insts_count;
  uint64_t page_faults_count;
  read(remote_cache_fd, &remote_cache_count, sizeof(remote_cache_count));
  read(memory_reads_fd, &memory_reads_count, sizeof(memory_reads_count));
  read(loads_fd, &loads_count, sizeof(loads_count));
  read(inst_fd, &insts_count, sizeof(insts_count));
  read(page_faults_fd, &page_faults_count, sizeof(page_faults_count));
  printf("\n");
  printf("%-80s = %15" PRIu64 "\n", "Page faults count", page_faults_count);
  printf("%-80s = %15.3f \n",       "time (milliseconds)", elapsedTime);
  if (access_mode == access_seq) {
    printf("%-80s = %15" PRId64 " (expected = %" PRId64 " considering each cache line is loaded once only)\n", "64 bytes cache line reads from RAM count (uncore event: QMC_NORMAL_READS.ANY)", memory_reads_count, (size_in_bytes / 64));
  } else {
    printf("%-80s = %15" PRId64 "\n", "64 bytes cache line reads from RAM count (uncore event: QMC_NORMAL_READS.ANY)", memory_reads_count);
  }
  printf("%-80s = %15" PRId64 "\n", "instructions count (core event: INST_RETIRED.ANY)", insts_count);
  printf("%-80s = %15" PRId64 " (expected = %d)\n", "loads count (core event: MEM_INST_RETIRED.LOADS)", loads_count, nb_elems);
  printf("%-80s = %15" PRId64 "\n", "remote cache count (core event: OFF_CORE_RESPONSE_0)", remote_cache_count);

  if (metadata_page->data_head > mmap_len) {
    printf("more samples than space in mmap allocated ring buffer\n");
    return -1;
  }
  print_samples(metadata_page, ADDR, (uint64_t)memory, (uint64_t)memory + size_in_bytes, nb_elems / period);
  close(remote_cache_fd);
  close(memory_sampling_fd);
  close(memory_reads_fd);
  close(loads_fd);
  close(inst_fd);
  close(page_faults_fd);
  if (numa_available() == -1 && NUMA_ALLOC) {
    free(memory);
  } else {
    numa_free(memory, size_in_bytes);
  }
  return 0;
}

void usage(const char *prog_name) {
  printf ("Usage %s size access_mode period\n\trun benchmarks where:\n\t\tsize is the size of the allocated and accessed memory in mega bytes\n\t\taccess_mode is either seq for sequential accesses or rand for random accesses\n\t\tperiod the sampling period in number of events\n", prog_name);
}

int main(int argc, char **argv) {

  /**
   * Pin process on core CPU
   */
  cpu_set_t mask;
  CPU_ZERO(&mask);
  CPU_SET(CPU, &mask);
  if (sched_setaffinity(0, sizeof(mask), &mask) == -1) {
    printf("sched_setaffinity failed: %s\n", strerror(errno));
    return -1;
  }

  /**
   * Check and get arguments.
   */
  if (argc <= 3) {
    usage(argv[0]);
    return -1;
  }
  size_t size_in_bytes = atol(argv[1]) * 1000000;
  enum access_mode_t access_mode;
  if (!strcmp(argv[2], "seq")) {
    access_mode = access_seq;
  } else if (!strcmp(argv[2], "rand")) {
    access_mode = access_random;
  } else {
    printf("Unknown access_mode %s\n", argv[2]);
    usage(argv[0]);
    return -1;
  }
  uint64_t period = atol(argv[3]);
  return run_benchs(size_in_bytes, access_mode, period);
}
